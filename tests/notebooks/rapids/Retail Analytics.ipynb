{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1c4327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import broadcast, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27643156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# You need to update these to your real paths!\n",
    "dataRoot = os.getenv(\"DATA_ROOT\", 'file:///run/determined/workdir/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91552966-e966-4765-9bf2-3c5455bb9fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "if \"sc\" in globals():\n",
    "    sc.stop()\n",
    "    \n",
    "conf = SparkConf().setAppName(\"Retail Analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d82ca0-8f78-4c48-886b-7ae170484289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f75a13d1ad0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Configure the parameters based on your dataproc cluster ###\n",
    "conf.set(\"spark.scheduler.minRegisteredResourcesRatio\", \"0.0\")\n",
    "\n",
    "conf.set(\"spark.executor.instances\", \"8\")\n",
    "conf.set(\"spark.executor.cores\", \"4\")\n",
    "conf.set(\"spark.executor.memory\", \"8192m\")\n",
    "conf.set(\"spark.executor.memoryOverhead\", \"4915m\")\n",
    "conf.set(\"spark.yarn.executor.launch.excludeOnFailure.enabled\",True)\n",
    "\n",
    "conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "\n",
    "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
    "conf.set(\"spark.sql.broadcastTimeout\", \"700\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"300M\")\n",
    "conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128M\")\n",
    "conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionNum\", \"1\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8517519-7f85-426a-b7ef-51a2e87bfab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f75a13d1ad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Enable RAPIDS config\n",
    "\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "# conf.set(\"spark.task.resource.gpu.amount\", \"1\") # This is the part that's failing...\n",
    "\n",
    "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "conf.set(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"4096m\")\n",
    "conf.set(\"spark.rapids.memory.host.spillStorageSize\", \"4g\")\n",
    "conf.set(\"spark.rapids.sql.multiThreadedRead.numThreads\", \"40\")\n",
    "conf.set(\"spark.rapids.sql.castDecimalToString.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.castStringToTimestamp.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.expression.PercentRank\",False)\n",
    "conf.set(\"spark.rapids.sql.castDecimalToString.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.format.json.read.enabled\",True)\n",
    "conf.set(\"spark.rapids.sql.explain\",None)\n",
    "conf.set(\"spark.rapids.sql.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e040a48-9ee0-49e5-8ec9-7a688787bec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880ade60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken on GPU for Data Cleaning:  33.29042196273804\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "def clean_data(df):\n",
    "    # remove missing values\n",
    "    df = df.dropna()\n",
    "    # remove duplicate data\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(spark, format, file_path):\n",
    "    if format==\"csv\":\n",
    "        return spark.read.format(format).load(file_path,header=True)\n",
    "    else:\n",
    "        return spark.read.format(format).load(file_path)\n",
    "\n",
    "# read sales data\n",
    "sales_df = read_data(spark, \"csv\", dataRoot+\"/raw/sales/\")\n",
    "\n",
    "# read stock data\n",
    "stock_df = read_data(spark, \"json\", dataRoot+\"/raw/stock/\")\n",
    "\n",
    "# read supplier data\n",
    "supplier_df = read_data(spark, \"json\", dataRoot+\"/raw/supplier/\")\n",
    "\n",
    "# read customer data\n",
    "customer_df = read_data(spark, \"csv\", dataRoot+\"/raw/customer/\")\n",
    "\n",
    "# read market data\n",
    "market_df = read_data(spark, \"csv\", dataRoot+\"/raw/market/\")\n",
    "\n",
    "# read logistic data\n",
    "logistic_df = read_data(spark, \"csv\", dataRoot+\"/raw/logistic/\")\n",
    "\n",
    "\n",
    "# data cleaning\n",
    "sales_df = clean_data(sales_df)\n",
    "stock_df = clean_data(stock_df)\n",
    "supplier_df = clean_data(supplier_df)\n",
    "customer_df = clean_data(customer_df)\n",
    "market_df = clean_data(market_df)\n",
    "logistic_df = clean_data(logistic_df)\n",
    "\n",
    "\n",
    "# convert date columns to date type\n",
    "sales_df = sales_df.withColumn(\"date_of_sale\", to_date(col(\"date_of_sale\")))\n",
    "stock_df = stock_df.withColumn(\"date_received\", to_date(col(\"date_received\")))\n",
    "supplier_df = supplier_df.withColumn(\"date_ordered\", to_date(col(\"date_ordered\")))\n",
    "\n",
    "# standardize case of string columns\n",
    "sales_df = sales_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", upper(col(\"location\")))\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", upper(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "\n",
    "# remove leading and trailing whitespaces\n",
    "sales_df = sales_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", trim(col(\"location\")))\n",
    "\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", trim(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "\n",
    "# check for invalid values\n",
    "sales_df = sales_df.filter(col(\"product_name\").isNotNull())\n",
    "stock_df = stock_df.filter(col(\"location\").isNotNull())\n",
    "customer_df = customer_df.filter(col(\"gender\").isin(\"male\",\"female\"))\n",
    "market_df = market_df.filter(col(\"product_name\").isNotNull())\n",
    "logistic_df = logistic_df.filter(col(\"product_name\").isNotNull())\n",
    "\n",
    "#drop extra columns\n",
    "market_df = market_df.drop(\"price\")\n",
    "supplier_df = supplier_df.drop(\"price\")\n",
    "\n",
    "# join all data\n",
    "data_int = sales_df.join(stock_df, \"product_name\",\"leftouter\").join(supplier_df, \"product_name\",\"leftouter\").join(market_df, \"product_name\",\"leftouter\").join(logistic_df, \"product_name\",\"leftouter\").join(customer_df, \"customer_id\",\"leftouter\")  \n",
    "\n",
    "# write the cleaned data\n",
    "data_int.write.format(\"parquet\").mode(\"overwrite\").save(dataRoot+\"/cleaned/\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken on GPU for Data Cleaning: \", end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa25c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|  sup_id|        avg(price)|\n",
      "+--------+------------------+\n",
      "|s_489730| 49.52406329057808|\n",
      "|s_406709| 69.81079196518795|\n",
      "|s_115079| 85.94399912664427|\n",
      "|s_344666| 97.59009379382898|\n",
      "|s_285915|14.294712207062046|\n",
      "| s_76565| 5.437999719320045|\n",
      "|s_140303| 87.53240260474355|\n",
      "|s_476437|60.546143121865114|\n",
      "|s_119978| 69.73744545566842|\n",
      "|s_236217| 46.73458879110069|\n",
      "|s_320400| 82.35581718551407|\n",
      "|  s_4063| 99.77199821260102|\n",
      "|s_157361| 49.94368539159035|\n",
      "| s_65960|  44.7196913589874|\n",
      "|s_465221| 39.55720484422409|\n",
      "|s_268747| 41.03514491012048|\n",
      "| s_20835|34.089356272623064|\n",
      "|s_495773| 85.89353051404636|\n",
      "|s_400631| 1.130315066427185|\n",
      "|s_226779| 46.68096597421045|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+------------------+----------------------+\n",
      "|  sup_id|        sum(price)|sum(quantity_in_stock)|\n",
      "+--------+------------------+----------------------+\n",
      "|s_489730| 49.52406329057808|                   462|\n",
      "|s_406709| 69.81079196518795|                   155|\n",
      "|s_115079| 85.94399912664427|                   989|\n",
      "|s_344666| 97.59009379382898|                   748|\n",
      "|s_285915|14.294712207062046|                   125|\n",
      "| s_76565| 5.437999719320045|                   302|\n",
      "|s_140303| 87.53240260474355|                   106|\n",
      "|s_476437|60.546143121865114|                   365|\n",
      "|s_119978| 69.73744545566842|                   987|\n",
      "|s_236217| 46.73458879110069|                   829|\n",
      "|s_320400| 82.35581718551407|                   172|\n",
      "|  s_4063| 99.77199821260102|                   579|\n",
      "|s_157361| 49.94368539159035|                   549|\n",
      "| s_65960|  44.7196913589874|                     8|\n",
      "|s_465221| 39.55720484422409|                   789|\n",
      "|s_268747| 41.03514491012048|                   398|\n",
      "| s_20835|34.089356272623064|                   462|\n",
      "|s_495773| 85.89353051404636|                   866|\n",
      "|s_400631| 1.130315066427185|                   501|\n",
      "|s_226779| 46.68096597421045|                   554|\n",
      "+--------+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----------------+\n",
      "|perishable|count(perishable)|\n",
      "+----------+-----------------+\n",
      "|        no|          9958915|\n",
      "|       yes|            41085|\n",
      "+----------+-----------------+\n",
      "\n",
      "+------------+-------------------+\n",
      "|sales_status|count(sales_status)|\n",
      "+------------+-------------------+\n",
      "|         bad|            4999147|\n",
      "|        good|            5000853|\n",
      "+------------+-------------------+\n",
      "\n",
      "0\n",
      "Time taken on GPU for Data Analysis:  17.771040439605713\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#DO VARIOUS RETAIL DATA ANALYTICS \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# read cleaned data\n",
    "\n",
    "data = spark.read.format(\"parquet\").load(dataRoot+\"/cleaned/\")\n",
    "\n",
    "#Case when statement to create a new column to indicate whether the product is perishable or not:\n",
    "\n",
    "data = data.withColumn(\"perishable\", when(col(\"shelf_life\") <= 30, \"yes\").otherwise(\"no\"))\n",
    "\n",
    "# You can use the when() and otherwise() functions to create new columns based on certain conditions:\n",
    "\n",
    "data = data.withColumn(\"sales_status\", when(col(\"quantity_sold\") > 50, \"good\").otherwise(\"bad\"))\n",
    "\n",
    "# create a window to perform time series analysis\n",
    "window = Window.partitionBy(\"product_name\").orderBy(\"date_of_sale\")\n",
    "\n",
    "# calculate the rolling average of sales for each product\n",
    "time_series_df = data.withColumn(\"rolling_avg_sales\", avg(\"quantity_sold\").over(window))\n",
    "\n",
    "# use window function for forecasting\n",
    "\n",
    "forecast_df = time_series_df.withColumn(\"prev_sales\", lag(\"rolling_avg_sales\").over(window))\\\n",
    "    .withColumn(\"next_sales\", lead(\"rolling_avg_sales\").over(window))\n",
    "\n",
    "\n",
    "# Calculate the average price of a product, grouped by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"price\": \"avg\"}).show()\n",
    "\n",
    "\n",
    "# Calculate the total quantity in stock and total sales by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"quantity_in_stock\": \"sum\", \"price\": \"sum\"}).show()\n",
    "\n",
    "#Calculate the number of perishable v/s non-perishable product per location\n",
    "forecast_df.groupBy(\"perishable\").agg({\"perishable\": \"count\"}).show()\n",
    "\n",
    "\n",
    "#Calculate number of good v/s bad sales status per location\n",
    "forecast_df.groupBy(\"sales_status\").agg({\"sales_status\": \"count\"}).show()\n",
    "\n",
    "# Count the number of sales that contain a 10% off promotion\n",
    "countt = forecast_df.filter(forecast_df[\"contains_promotion\"].contains(\"10% off\")).count()\n",
    "print(countt)\n",
    "# Perform some complex analysis on the DataFrame\n",
    "\n",
    "# Calculate the total sales, quantity sold by product and location\n",
    "total_sales_by_product_location = forecast_df.groupBy(\"product_name\", \"location\").agg(sum(\"price\").alias(\"total_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_sold\"),avg(\"quantity_sold\").alias(\"avg_quantity_sold\")).sort(desc(\"total_price\"))\n",
    "\n",
    "# Group the data by product_name\n",
    "grouped_df = forecast_df.groupBy(\"product_name\")\n",
    "\n",
    "#Sum the quantity_in_stock, quantity_ordered, quantity_sold, and (price * quantity_sold) for each group\n",
    "aggregated_df = grouped_df.agg(sum(\"quantity_in_stock\").alias(\"total_quantity_in_stock\"),avg(\"price\").alias(\"average_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_ordered\"),sum(\"quantity_sold\").alias(\"total_quantity_sold\"),sum(col(\"price\") * col(\"quantity_sold\")).alias(\"total_sales\"),sum(\"prev_sales\").alias(\"total_prev_sales\"),sum(\"next_sales\").alias(\"total_next_sales\"),).sort(desc(\"total_sales\"))\n",
    "\n",
    "#WRITE THE AGGREGATES TO DISK\n",
    "aggregated_df.write.format(\"parquet\").mode(\"overwrite\").save(dataRoot+\"/app/data.parquet\")\n",
    "total_sales_by_product_location.write.format(\"parquet\").mode(\"overwrite\").save(dataRoot+\"/app1/data.parquet\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken on GPU for Data Analysis: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519dafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a154a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
